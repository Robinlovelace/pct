---
title: Estimating 'distance decay' functions for modelling transport systems using
  individual and aggregate-level flow data
author: "Robin Lovelace"
date: "February 13, 2015"
output:
  pdf_document:
    fig_caption: yes
    latex_engine: xelatex
    number_sections: yes
    toc: yes
bibliography: ~/Documents/Transport.bib
---

```{r, include=FALSE}
# output: word_document
# For reproducible results see https://github.com/Robinlovelace/pct
pkgs <- c("png", "grid", "ggmap", "ggthemes", "knitr", "nplr")
lapply(pkgs, library, character.only = TRUE)
```

Distance decay (*DD*) is
the relationship between the distance
of a trip and the probability of it being made by a particular mode.
A clear understanding of *DD* is advantageous in modelling modal shift and
other changes in transport systems because it can pave the way to
estimating the probability of one form of transport replacing another.
*DD* can also be used to estimate the probability of a trip happenning at all.
This methodological paper explores the concept of *DD*, demonstrates techniques
for 'curve fitting' based on individual and aggregate-level 'flow' and
travel diary datasets (both common in countries around the world), evaluates
various functional forms for describing *DD* and provides recommendations for
further work. Critically for other researchers wanting to use the concept of
*DD* in their own work, the methods presented here are completely reproducible
using publicly available data and open source sofware.

The aim is to describe some new developments in understandings of how distance
affects travel using a probabilistic modelling framework. Based on
reliable estimates of *DD* curves, many probabilistic modelling methods are
opened-up, allowing for a range of scenarios to envisioned, operationalised and
quantified. It is not the purpose of this paper to describe the full suite of
options that *DD* estimates open up. We refrain from heralding a new 'distance
decay paradigm'. However, the reason for developing methods of estimating *DD* 
curves were closely related to modelling needs and we will discuss how robust
estimates of *DD* can be used for modelling in the final section of the paper.
Primarily the paper is methodological, however, hence our use of a relatively
small example dataset of no particular empirical interest.

# Conceptualising distance decay

Clearly
(as illustrated by the growth of long-distance bicycle touring) *all* trips
are cyclable in theory. In practice, however, short trips are far more likely
to be cycled than longer trips for reasons including time, stamina and comfort.
This fact is represented in the flow data (Figure 1). The challenge is extracting the
nature of *DD* from the flow data, which is rather opaque in its raw form.

```{r, message=FALSE, fig.cap="Plot of the demonstration city (Leeds) with all cycle trip flows represented by the white lines (oppacity is proportional to bicycle commuters).", echo=FALSE}
source("case-studies/leeds.R")
img <- readPNG("bigdata/figures/leeds-flow-bicycle.png")
grid.raster(img)
```

Building on 'the principle of parsimony' --- otherwise known as Occam's razor ---
we will start with the simplest model that explains *DD* in active
travel reasonably well This is log-linear decay, characterised by two parameters
(Iacono, 2010). We will build on this example to explore alternative
distance-decay functions.

# Estimating distance decay

A problem with the log-linear function
(and any log-model) for fitting to raw flow data on cycling is that
the log of zero is minus infinity ($log(0) = -\infty$) yet 0 flows between
origins and destinations are common for all modes of transport, and especially
so for rare modes. Of the 589 positive commuter flow lines between 25 MSOA zones 
surrounding Leeds city centre, for example, more than half (389) have 0 cyclists.
Zero and one-inflation
clearly poses a problem to efforts to parameterise *DD* through linear
regression: it is hard to estimate the parameters that result in infinity!

In addition, because flow matrices are so sparse, counts of 1 are common.
Thus both extreme values of
0 and 1 values are excessively common (or *inflated*) in raw flow data, 
as illustrated in Figure 2. This means non-probabilistic
regression models will have low overall goodness-of-fit when fitted to the
raw data and that weighted regression must be used to prevent outliers having
a disproportionate impact on the results.

```{r, fig.height=3, fig.width=4, fig.cap="Scatterplot of flow distance and proportion cycling illustrating the frequency of 0s and 1s in the proportion of commutes by bicycle. Data: Leeds (n = 10,242).", echo=FALSE}
flow$pcycle <- flow$Bicycle / flow$All
plot(flow$dist, flow$pcycle, xlab = "Distance (km)", ylab = "Proportion of commutes by bike") 
# the problem with flow data: many 0's and 1's
```

As we shall see, *binning by distance* is a solution to this problem that
has the associated benefit of greatly reducing the computational requirements
of the regression. Binning involves aggregating the data from one level
to another to estimate the *average*
(or population-weighted average) proportion of trips made by bicycle.
Binning is also advantageous in that it demonstrates visually how well
any given DD function fits the data. A downside is that 
the binning approach cannot predict
the rate of flow between individual OD pairs, so cannot realistically be
used to estimate *ECP*, dependent on zone characteristics,
for OD flows. 

## Binning flows by distance

### Binning distances

Distance in flow data is calculated as the Euclidean distance
(a refinement would be to use route distance)
between origin and destination centroids and is a truly continuous variable.
This differs from the individual-level survey data which treats distance
as a quasi-continuous variable with a granularity of 0.1 miles (and clumping
around integer distances, which impacts on our selection of distance bins).
We bin the distances as follows, and set the maximum distance to 20.5 miles:

```{r}
brks <- c(0, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 9.5, 12.5, 15.5, 20.5)
flow <- flow[flow$dist < 20.5, ]
```

Note that the bins were selected to contain integer distances. Althought this makes
no difference for continuous flow data, it helps reduce the impact of rounding
to the nearest integer by respondents in travel surveys. This binning strategy
eases the translation of the methods from aggregate flow to individual trip-level
data, as we shall see below. 
The distance bins allow all OD flows to be grouped by distance and enable the creation
of aggregates for each distance band:

```{r}
flow$binned_dist <- cut(flow$dist, breaks = brks, include.lowest = T)

# Create aggregate variables
gflow <- group_by(flow, binned_dist) %>%
  summarise(dist = mean(dist), mbike = mean(pcycle),
    total = sum(All))
gflow$nbike <- gflow$total * gflow$mbike / 100 # number of bikes in each zone
```

### Defining average distances per bin

The simplest approach to setting the distance associated with each bin
score is to take the mid-point between the upper and lower value of each bin:

```{r}
brks_min <- head(brks, -1)
brks_max <- brks[-1]
(midpoint <- (brks_min +brks_max) / 2)
```

However this is an oversimplification: distance distributions are strongly
positively skewed so the average of distances within the further distance bands
are likely to be lower than the mean. This is true of our case
study city (Figure 3). Therefore we use the computed mean distance for each bin:

```{r, fig.height=3, fig.width=4, fig.cap="The relationship between bin midpoint and average Euclidean distance of trips within each bin."}
plot(gflow$dist, midpoint, xlab = "Mean distance (km)")
abline(a = 0, b = 1)
```

The overall number of trips per distance band is illustrated in Figure 4.
The barely visible grey fill at the bottom of the bars represents bicycle
trips. Unsurprisingly, these all-but disappear after 6 km, demonstrating
that distance decay for cycling is faster than for trips overall.

```{r, echo=FALSE, fig.cap="Number of trips overall per distance band (white) and number of bicycle trips per distance band (grey)."}
tb <- sum(gflow$total)
ggplot(data = gflow) + 
  geom_histogram(aes(x = midpoint, y = ..density.. * 234968, weight = total),
  position = "identity", breaks = brks, fill = "white", colour = "black") +
    geom_histogram(aes(x = midpoint, y = ..density.. * 4648, weight = nbike),
  position = "identity", breaks = brks, alpha = 0.2) +
  ylab("Number of trips per bin") + xlab("Distance (km)") +
  theme_bw()
```

## Estimating distance decay from binned data using polynomials

Now the data have been grouped and assigned to approximate distances,
we can use regression to explore **DD** for cycling.
The relationship is strikingly curvilinear in the example city.
With this non-linear relationship in mind (with a peak around 3 km),
we fitted various log-linear, quadratic and cubic functional forms to the data.

```{r}
mod_loglin <- lm(log(mbike) ~ dist, data = gflow)
# mod_logsqr <- lm(log(mbike) ~ dist + I(dist^2), data = gflow)
mod_logsqr <- lm(log(mbike) ~ dist + I(dist^0.5), data = gflow, weights = total)
mod_logcub <- lm(log(mbike) ~ dist + I(dist^2) + I(dist^3), data = gflow)
mod_cub <- lm(mbike ~ dist + I(dist^2) + I(dist^3), data = gflow)
```

The above code fitted log-linear, log-square and log-cubic functional forms to the
data. We also ran the same models using *weighted regression*, setting the
weight for each OD flow proportional to the total (all modes) rate of travel.
Adding weights to the regression in this way reduces the impact of
the longer-distance commutes, which fewer cyclists make. 
The results of these regressions are presented in Figure 5.

```{r, echo=FALSE}
modw_logcub <- lm(log(mbike) ~ dist + I(dist^2) + I(dist^3),
  weights = total, data = gflow)
modw2_logcub <- lm(log(mbike) ~ dist + I(dist^2) + I(dist^3),
  weights = nbike, data = gflow)
```

```{r, fig.cap="Linear regression to estimate *DD* from flow data. Blue, red and green lines represent log-linear, quadratic and cubic functional forms, respectively. The dotted and dashed green lines represent the impact of weighting but the number of total and cyclist commuter trips in each category, respectively.  The black line is the cubic regression, without taking the log of y.", echo=FALSE}
plot(gflow$dist, gflow$mbike,
  xlab = "Distance (km)", ylab = "Proportion cycling", ylim = c(0, 0.03))
lines(gflow$dist, exp(mod_loglin$fitted.values), col = "blue")
lines(gflow$dist, exp(mod_logsqr$fitted.values), col = "red")
lines(gflow$dist, exp(mod_logcub$fitted.values), col = "green")
lines(gflow$dist, exp(modw_logcub$fitted.values), col = "green", lty = 2)
lines(gflow$dist, exp(modw2_logcub$fitted.values), col = "green", lty = 3)
lines(gflow$dist, mod_cub$fitted.values, col = "black", lty = 4)
```

Does the log term make any difference to model fit? These and other questions
are answered quantitatively in Table 1 below. The log terms
does increase model fit for the example dataset.
From visual inspection of the models, the log-cubic model clearly fits best
and this is apparent from the adjusted R-squared scores
(which takes accound of over-fitting through excessive free parameters).
The log-cubic model explains around 90% of variation in the average
proportion of people who cycle in different distance groups, while the
log-linear and log-quadratic models barely explain half the variability.

```{r, eval=FALSE, echo=FALSE}
mnames <- ls()[grep("mod_", ls())]
mnames <- mnames[-grep("logi", mnames)]
mods <- data.frame(Model = mnames, Adj.R.sq = NA)
for(i in 1:nrow(mods)){
  mods$Adj.R.sq[i] <- summary(get(mnames[i]))$adj.r.squared
  # round(summary(get(mnames[i]))$coef, digits = 3 )[,1]
}
kable(mods, digits = 3)
```

Table 1: Adjusted R-squared scores of various models. mod_ind* models
were fitted to the raw flow data, hence their much poorer fit.

|Model      | Adj.R.sq|
|:----------|--------:|
|mod_cub    |    0.784|
|mod_ind1   |    0.013|
|mod_ind2   |    0.041|
|mod_ind3   |    0.041|
|mod_logcub |    0.907|
|mod_loglin |    0.591|
|mod_logsqr |    0.569|

```{r, echo=FALSE}
# summary(modw_logcub)
# plot(gflow$dist, gflow$mbike,
#   xlab = "Distance (km)", ylab = "Proportion cycling")
# lines(gflow$dist, exp(modw_loglin$fitted.values), col = "blue")
# lines(gflow$dist, exp(modw_logsqr$fitted.values), col = "red")
# ```{r, fig.cap="Weighted regression results to estimate *DD* from flow data. Blue, red and green lines represent log-linear, quadratic and cubic functional forms, respectively."}
# old <- setwd("../")
# img <- readPNG("figures/weighted-reg.png")
# grid.raster(img)
# ```
```


```{r, echo=FALSE}

# 
# 
# ## Zero-inflated regression
```


## A simplified 'Planck function'

The dotted curve in Figure 6 illustrates a problem with the cubic distance
decay function: after reaching a minimum value it lurches upwards. Thus, in
all versions of this model, the proportion of people cycling 20 km to work
is greater than the proportion cycling 10 km. The dotted line shows that, even
after weighting the flows so the more common shorter trips have a disproportionate
impact on the model, the relatively rare longer trips hold great sway over
the shape of the DD curve produced by the cubic functions. This reduces the
fit of the model and, worse, means that DD curves in regions with no long-distance
trips will be very different from those produced in regions with long-distance
trips, even if the underlying pattern is identical.

A function that always tends to zero as distance tends to infinity would be
preferable. An equation in which the dependent variable
rises rapidly and then decays gradually with an independent variable is the
'Planck Law':

A simplified version of this equation was developed after some experimentation
to fit to the data:

$$
pc= \frac{a (b + d)}{e^{k(b + d)}}
$$

In words, this means that *pc* increases (how rapidly depends on the value of *b*)
to some maximum value (dependent on *a*) before entering a exponential decay
phase, the gradient of which dominated by *k*.

This function can be easily written in R and fitted to the data using the
Nonlinear Least Squares function `nls`.
The function can also be fitted to the flow-level data.

```{r, echo=FALSE}
# The original form of the Planck-like dd function:
dd_planck_orig <- function(x, a, b, k){
  (a * (x^b)) / (exp(k * x))
}
```


```{r}
dd_planck <- function(x, a, b, k){
  (a * (b + x)) / (exp(k * (b + x)))
}

mod_plank1 <- nls(mbike ~ dd_planck(dist, a, b, k), weights = total,
  data = gflow, start = list(a = 0.1, b = 0.1, k = 0.1))
```


```{r, echo=FALSE}
mod_plank2 <- nls(pcycle ~ dd_planck(dist, a, b, k), weights = All,
  data = flow, start = list(a = 0.1, b = 0.1, k = 0.1))

mod_plank3 <- nls(mbike ~ dd_planck_orig(dist, a, b, k), weights = total,
  data = gflow, start = list(a = 0.1, b = 0.1, k = 0.1))
```

Figure 6 compares the model fit acheived with this 'Planck' function against
that acheived with the log-cubic function. It was found that, despite the anomolous
upward tick in the example data (which would benefit the log-cubic function),
the Planck-like function fits the data better. Non-adjusted R-squared values
of 0.85 and 0.89 were obtained from the Planck and log-cubic functions respectively.

```{r, echo=FALSE, fig.cap="The fitted 'Planck function' (continuous line). The dotted line, for comparison, is the log-cubic function described in the previous section."}
x = seq(0.1, 20, by = 0.1)
df <- data.frame(dist = x)
df2 <- data.frame(dist = x)
df <- data.frame(dist = x)
plot(gflow$dist, gflow$mbike,
  xlab = "Distance (km)", ylab = "Proportion cycling", ylim = c(0, 0.03))
lines(df$dist, predict(mod_plank1, df), col = "blue", lwd = 4)
# lines(df$dist, predict(mod_plank3, df), col = "blue")
lines(df$dist, exp(predict(mod_logsqr, df)), col = "blue")
# lines(df$dist, predict(mod_plank2, df), col = "red")
# lines(df$dist, predict(object = mod_ind2, df), col = "red", lty = 2)
lines(df2$dist, exp(predict(object = modw_logcub, df2)),
  col = "blue", lty = 5)
# cor(mod_plank1$m$fitted(), gflow$mbike)^2
# cor(mod_plank3$m$fitted(), gflow$mbike)^2
# cor(modw_logcub$fitted, gflow$mbike)^2
```

## Interpretting the parameters of the Planck function

Figure 7 illustrates the impact of varying the parameters *a*, *b* and *k*
on the Planck function. It is shown that these parameters allow scenarios of
future changes in DD to be changed according to knowledge of growth in the
willingness to cycle long distances (decreases in *k*), shifts in the distance
of the peak rate of cycling (*b*) and proportional increases in cycling for all
distances.

```{r, echo=FALSE, fig.cap="Model experiments illustrating the impacts of altering a, b and k parameters of the 'Planck' distance decay function. For a and k the green lines represent increases of 25% of the original value derived from the Leeds dataset and the red lines represent decreases of 10%. For b they represent changes of +2 and -1 respectively."}
pars <- coef(mod_plank1)
mults_plus <- cbind(diag(3) * c(0.25, 2, 0.25) + 1, "Increase", c("a", "b", "k"))
mults_minus <- cbind(diag(3) * -c(0.1, 1, 0.1) + 1, "Decrease", c("a", "b", "k"))
mults_orig <- cbind(matrix(1, 3, 3), "Original", c("a", "b", "k"))
mults <- rbind(mults_minus, mults_plus, mults_orig)
dfp <- NULL
for(i in 1:9){
  ptemp <- as.numeric(mults[i, 1:3]) * pars
  dftemp <- data.frame(
    Distance = x,
    PC = dd_planck(x, a = ptemp[1], b = ptemp[2], k = ptemp[3]),
    Experiment = mults[i, 4],
    Parameter = mults[i, 5]
  )
  dfp <- rbind(dfp, dftemp)
}
ggplot(dfp, aes(x = Distance, y = PC, color = Experiment)) + 
  geom_line() +
  facet_wrap(~ Parameter) +
  scale_color_manual(values = c("red", "green", "black")) +
  theme_bw()
```


```{r, echo=FALSE, eval=FALSE}
# comparison for mod_logsqr model
dd_logsqr <- function(distance, intercept, loglin, logsqrt = 0){
  exp(intercept + loglin * distance + logsqrt * distance^0.5)
}

pars <- coef(mod_logsqr)
mults_plus <- cbind(diag(3) * c(0.25, 0.25, 0.25) + 1, "Times 1/4", c("Intercept (-4.2)", "Log-lin (-0.2)", "Log-sqrt (0.7)"))
mults_minus <- cbind(diag(3) * -c(0.1, 0.1, 0.1) + 1, "Times -0.1", c("Intercept (-4.2)", "Log-lin (-0.2)", "Log-sqrt (0.7)"))
mults_orig <- cbind(matrix(1, 3, 3), "Original", c("Intercept (-4.2)", "Log-lin (-0.2)", "Log-sqrt (0.7)"))
mults <- rbind(mults_minus, mults_plus, mults_orig)
dfp <- NULL
for(i in 1:9){
  ptemp <- as.numeric(mults[i, 1:3]) * pars
  dftemp <- data.frame(
    Distance = x,
    PC = dd_logsqr(x, ptemp[1], ptemp[2], ptemp[3]),
    Experiment = mults[i, 4],
    Parameter = mults[i, 5]
  )
  dfp <- rbind(dfp, dftemp)
}
ggplot(dfp, aes(x = Distance, y = PC, color = Experiment)) + 
  geom_line() +
  facet_wrap(~ Parameter) +
  scale_color_manual(values = c("red", "green", "black")) +
  theme_bw()
# ggsave("figures/log-sqrt-params.png")
```


# Estimating propensity to cycle based on individual flows

There are issues with fitting the regression model to the grouped data.
Using grouped data precludes the inclusion of flow-level variables that could
be useful in acurately estimating the potential level of cycling (CLC)
such as steepness and the characteristics of the origin zone. This is not
a problem as we can fit exactly the same models presented in the previous
section to the raw flow data, which is messier (Figure 8).

```{r, echo=FALSE}
flow$clc <- flow$Bicycle / flow$All
mod_ind1 <- lm(pcycle ~ dist + I(dist^2) + I(dist^3), data = flow)
mod_ind2 <- lm(pcycle ~ dist + I(dist^2) + I(dist^3), data = flow,
  weights = All)
f3 <- flow[flow$dist < 15, ]
mod_ind3 <- lm(pcycle ~ dist + I(dist^2) + I(dist^3), data = f3,
  weights = All)
```

```{r, fig.cap="Distance decay curves based on the cubic polynomial regression against distance for all flows (red lines). The continuous and dashed lines represent unweighted, and weighted implementations. The dotted lines is fitted only to flows of < 15 km. The green line is the fit produced from the binned data.", echo=FALSE}

plot(flow$dist, flow$pcycle, xlim = c(0, 16), ylim = c(0, 0.04),
  xlab = "Distance (km)", ylab = "Percentage cycling")
lines(df$dist, predict(object = mod_ind1, df), col = "red", lwd = 4)
lines(df$dist, predict(object = mod_ind2, df), col = "red", lwd = 6, lty = 2)
lines(df$dist, predict(object = mod_ind3, df), col = "red", lwd = 6, lty = 3)
lines(df2$dist, exp(predict(object = modw_logcub, df2)),
  col = "green", lwd = 5)
# lines(gflow$dist, mod_cub$fitted.values, col = "black", lty = 4, lwd = 6)
```

# Regression against the cumulative probability of cycling

To reduce the noise in the data arising from 0s and 1s, the rate of
cycling can be viewed as a cumulative distribution of trips with distance:

```{r}
fc <- flow[ order(flow$dist), ] 
head(fc$dist)
head(cumsum(fc$Bicycle)) # cumulative distances
fc$cumclc <- cumsum(fc$Bicycle) / sum(fc$Bicycle)
```

We can fit a  logistic regression model to this data, with the following
functional form:

```{r}
# plot(fc$dis, fc$cumclc)
mod_logist1 <- glm(cumclc ~ dist, family = "binomial",
  weights = fc$All, data = fc)
y1 <- mod_logist1$fitted.values

# lines(fc$dist, y1, col = "red", lwd = 4)
summary(mod_logist1) # how to convert these params to dd?
```

Note the quality of the fit and the simplicity of the data, compared
with the non-cumulative distribution of proportion cycling with distance.
The challenge is converting this into a distance decay function and
allowing for *skew* in the rate of uptake. These things can be represented
in a more general version of the logistic equation, commonly known as the 
Richard's equation [@Richards1959]:

$$y = b + \dfrac{t - b}{\left [1 + 10^{r(x_{mid} - x)} \right ]^s}$$

$b$ and $t$ are the bottom and top asymptotes, and $r$, $x_{mid}$ and $s$ are the maximum slope, the x-coordinate at the inflexion point and an asymetric coefficient, respectively.

In R, this equation can be written as follows:

```{r, fig.cap="5 parameter logistic model"}
log5p <- function(x, b = 0, t = 1, xmid = 0, r = 1, s = 1, l = 1){
  b + (t - b) / (1 + 10 ^(r * (xmid - l * x)))^s
}
x = seq(0, 20, 0.1)
ytest = log5p(x, 0, 1, 5, 1, 1)
# plot(x, y) # plot not shown
```

```{r, echo=FALSE}
# Demonstrate this function in action - what it depends on and all...
```

The differential of this equation is displayed below, demonstrating the
link between cumulative probability probability density. 

$$
 \mathrm{log}(10) \,r \,s\,\left( T - B\right) \,{10}^{r\,\left( xmid - x\right) }\,{\left( {10}^{r\,( xmid-x) }+1\right) }^{-s-1}
$$

Converting this equation into an R function, we can plot the differential of the
curve described above:

```{r}
log5p_diff <- function(x, b = 0, t = 1, xmid = 0, r = 1, s = 1){
  r * s * (t - b) * 10^(r * (xmid - x)) * (10^(r * (xmid - x)) + 1)^(-s-1)
}
dd1 <- log5p_diff(x, 0, 1, 5, 1, 1)
# plot(x, ytest)
# lines(x, dd1)
```

Now we have demonstrated the link between the 5 parameter logistic
model and its differential, we can make the important observations:

1. Fitting a logistic regression model to the cumulative probability of trips
being made by cycling can result in an estimate of distance decay.
2. The *differential* of the Richard's allows for the calculation of model
parameters that are meaningful in terms of the rate of distance decay
($r$) and the absolute rate of cycling.

Let's fit the Richard's equation to the data. The differential form
can be fit directly to the raw data, while the logistic growth form can
be fitted to the cumulative probability form. In theory, we should get the
same result for the model parameters for both regression models.

```{r}
nls_5p1 <- nls(cumclc ~ 1 / (1 + 10 ^(r * (xmid - dist))),
  data = fc, start = list(xmid = 1, r = 0.2), 
  weights = fc$All)
nls_5p2 <- nls(cumclc ~ 1 / (1 + 10 ^(r * (xmid - dist)))^s,
  data = fc, start = list(xmid = 1, r = 0.2, s = 0.2),
  control = list(warnOnly =T), 
  weights = fc$All)

y3 <- predict(nls_5p1, fc)
y4 <- predict(nls_5p2, fc)
```


```{r}
# Plot the regressions
plot(fc$dist, fc$cumclc, type = "line")
lines(x, ytest, col = "yellow")
lines(fc$dist, y1, col = "green")
lines(fc$dist, y3, col = "red")
lines(fc$dist, y4, col = "blue")
```

To convert from the logistic regression results to distance decay,
simply extract the coefficients:

```{r, echo=FALSE, eval=FALSE}
(c1 <- coef(nls_5p1))
(c2 <- coef(nls_5p2))
dd2 <- log5p_diff(x, xmid =  c1[1], r = c1[2])
dd3 <- log5p_diff(x, xmid =  c2[1], r = c2[2], s = c2[3])

plot(flow$dist, flow$pcycle, xlim = c(0, 16), ylim = c(0, 0.1),
  xlab = "Distance (km)", ylab = "Percentage cycling")
lines(x, dd2 * 100, col = "red", lwd = 4)
lines(x, dd3 * 100, col = "green", lwd = 4)
lines(df$dist, predict(object = mod_ind2, df), col = "red", lwd = 6, lty = 2)
points(gflow$binned_dist, gflow$mbike, col = "blue", pch = 22)
# nls_5p1 <- nls(pcycle ~ b + (t - b) / (1 + 10 ^(r * (xmid - dist)))^s, data = fc, start = list(b = 0, t = 1, xmid = 5, r = 2, s = 1))
```



```{r, echo=FALSE, fig.cap="Illustration of differential of the Richard's equation fitted to the data (solid black: unweighted; dotted black: weighted)."}
# # Testing
# set.seed(20)
# pcycle <- rnorm(20, mean = 0.1, sd = 5)
# pcycle <- pcycle[ pcycle > 0 ]
# pcycle <- pcycle[ order(pcycle, decreasing = T)]
# fc2 <- data.frame(dist = 1:length(pcycle), pcycle)

# gamma distribution
fc3 <- fc
fc3$pcycle[fc3$pcycle == 0 ] <- 0.000001 # gamma dislikes zero values!
fc3$pcycle <- fc3$pcycle / 100
glm1 <- glm(pcycle ~ dist, family = Gamma(link = "identity"), data = fc3)
glm2 <- glm(pcycle ~ dist, family = Gamma("inverse"), data = fc3)

# gaussian
glm4 <- glm(pcycle ~ dist, family = gaussian, data = fc)

nls_5pd1 <- nls(pcycle ~ r * s * 10^(r * (dist)) * (10^(r * (dist)) + 1)^(-s-1), data = fc, start = list(r = 0.2, s = 1))

nls_5pd2 <- nls(pcycle ~ r * s * 10^(r * (dist)) * (10^(r * (dist)) + 1)^(-s-1), data = fc, start = list(r = 0.2, s = 1), weights = All)

# nls_5pd3 <- nls(pcycle ~ r * s * 10^(r * (dist * xmid)) * (10^(r * (dist * xmid)) + 1)^(-s-1), data = fc, start = list(xmid = 0.8, r = 0.2, s = 1), weights = All) 

nls_5pd4 <- nls(pcycle ~ r * s * 10^(r * (dist)^t) * (10^(r * (dist)) + 1)^(-s-1), data = fc, start = list(t = 1, r = 0.2, s = 1), weights = All)

nls_5pd5 <- nls(pcycle ~ r * s * 10^(r * (dist)) * (10^(r * (dist)) + 1)^(-s-1) + t, data = fc, start = list(t = 1, r = 0.2, s = 1), weights = All)

ynls1 <- predict(nls_5pd1, newdata = data.frame(dist = x)) 
ynls2 <- predict(nls_5pd2, newdata = data.frame(dist = x)) 
# ynls3 <- predict(nls_5pd3, newdata = data.frame(dist = x)) 
ynls4 <- predict(nls_5pd4, newdata = data.frame(dist = x)) 
ynls5 <- predict(nls_5pd5, newdata = data.frame(dist = x)) 

# Plotting
plot(fc$dist, fc$pcycle, xlab = "Distance (km)", ylab = "Proportion cycling", ylim = c(0, 0.04), xlim = c(0, 20))
lines(x, ynls1, lwd = 4, col = "red")
lines(x, ynls2, lwd = 4, lty = 2, col = "red")
# lines(x, ynls5, lwd = 4, lty = 2, col = "red")

# lines(x, ynls2, lwd = 4, lty = 3)
# lines(x, ynls4, lwd = 4, lty = 5) # no difference
lines(df$dist, predict(object = mod_ind1, df), col = "green", lwd = 4, lty = 2)
# lines(df$dist, predict(object = mod_ind2, df), col = "red", lwd = 2, lty = 2)
# lines(gflow$dist, exp(modw_logcub$fitted.values), col = "green", lty = 2, lwd = 6)
```

```{r, echo=FALSE, eval=FALSE}

# # # # # # # #
# Binned data #
# # # # # # # #

# nls_5pd1 <- nls(mbike ~ r * s * 10^(r * (dist)) * (10^(r * (dist)) + 1)^(-s-1), data = gflow, start = list(r = 0.1, s = 1))

nls_5pd2 <- nls(mbike ~ r * s * 10^(r * (dist)) * (10^(r * (dist)) + 1)^(-s-1), data = gflow, start = list(r = 0.2, s = 1), weights = total)

# nls_5pd3 <- nls(mbike ~ r * s * 10^(r * (dist * xmid)) * (10^(r * (dist * xmid)) + 1)^(-s-1), data = gflow, start = list(xmid = 0.8, r = 0.2, s = 1), weights = total) 

nls_5pd4 <- nls(mbike ~ r * s * 10^(r * (dist)^t) * (10^(r * (dist)) + 1)^(-s-1), data = gflow, start = list(t = 1, r = 0.2, s = 1), weights = total) 

nls_5pd5 <- nls(mbike ~ r * s * 10^(r * (dist)) * (10^(r * (dist)) + 1)^(-s-1) + t, data = gflow, start = list(t = 1, r = 0.2, s = 1), weights = total) 

# ynl1 <- predict(nls_5pd1, newdata = data.frame(dist = x)) 
ynl2 <- predict(nls_5pd2, newdata = data.frame(dist = x)) 
# ynl3 <- predict(nls_5pd3, newdata = data.frame(dist = x)) 
ynl4 <- predict(nls_5pd4, newdata = data.frame(dist = x)) 
ynl5 <- predict(nls_5pd5, newdata = data.frame(dist = x)) 

plot(gflow$dist, gflow$mbike,
  xlab = "Distance (km)", ylab = "Proportion cycling", ylim = c(0, 0.03))
# lines(x, ynl1, lwd = 4)
lines(x, ynl2, lwd = 4, lty = 2, col = "red")
# lines(x, ynls2, lwd = 4, lty = 3)
# lines(x, ynls4, lwd = 4, lty = 5) # no difference
lines(x, ynl5, lwd = 4, lty = 3, col = "blue") # no difference
# lines(df$dist, predict(object = mod_ind1, df), col = "red", lwd = 2)
# lines(df$dist, predict(object = mod_ind2, df), col = "red", lwd = 2, lty = 2)
lines(gflow$dist, exp(modw_logcub$fitted.values), col = "green", lty = 2, lwd = 3)
```


## Curve fitting with **nplr**

**nplr** is a package devoted to fitting the types of 'Richards'
curves, described above, to data [@Commo2014]. Below we see some examples
of the models that can be fitted, with
`npars` representing the number of parameters in the final model. 

```{r, eval=FALSE, echo=FALSE}
mod_logist2 <- nplr(fc$dist, fc$cumclc)
mod_logist3 <- nplr(fc$dist, fc$cumclc, useLog = F)
mod_logist4 <- nplr(fc$dist, fc$cumclc, npars = 2)
mod_logist5 <- nplr(fc$dist, fc$cumclc, npars = 3)
plot(mod_logist4) 
```

```{r, echo=FALSE}
img <- readPNG("figures/2-param-nplr.png")
grid.raster(img)
```

The best in terms of simplicity seems to be the 4th model:

```{r, eval = F}
mod_logist4
# Instance of class nplr
# 
# 2-P logistic model
# Bottom asymptote: 0 
# Top asymptote: 1 
# Inflexion point at (x, y): 3.426146 0.5 
# Goodness of fit: 0.9947746 
# Standard error: 0.0205036 

getPar(mod_logist4)
# $npar
# [1] 2
# 
# $params
#   bottom top     xmid     scal s
# 1      0   1 3.426146 3.192967 1
```

Because only the $b$ and $x_{mid}$ parameters are used in the above regression,
the equation can be simplified substantially, to:

$$y = 1 + 10^{b(x_{mid} - x)} $$

```{r}
y =  1 / (1 + 10^(3.2 * (3.4 - x)))
plot(x, y)
```

# References



```{r, echo=FALSE}
# Bumpf (code for the rubbish bin :)
# log5p_formula <- formula(log5p_diff)
# nls_5pd1 <- nls(pcycle ~ r * s * 10^(r * (xmid - dist)) * (10^(r * (xmid - dist)) + 1)^(-s-1), data = fc, start = list( xmid = 5, r = 0.2, s = 10)) # fail!

# nls_5pd1 <- nls(pcycle ~ r * s * (t - b) * 10^(r * (xmid - dist)) * (10^(r * (xmid - dist)) + 1)^(-s-1), data = fc2, start = list(b = 0, t = 1, xmid = 5, r = -0.2, s = 0.1)) 
# nls_5p1 <- nls(pcycle ~ r * t * 10^(r * (xmid - dist)) * (10^(r * (xmid - dist)) + 1), data = fc, start = list(t = 1, xmid = 5, r = 2))
```



