---
title: "Estimating extra cycling potential from aggretate flows"
author: "Robin Lovelace"
date: "February 13, 2015"
output:
  pdf_document:
    fig_caption: yes
    latex_engine: xelatex
bibliography: ~/Documents/Transport.bib
---

```{r, include=FALSE}
# output: word_document
# For reproducible results see https://github.com/Robinlovelace/pct
pkgs <- c("png", "grid", "ggmap", "ggthemes", "knitr", "nplr")
lapply(pkgs, library, character.only = TRUE)
opts_knit$set(root.dir = "../")
```

There are many ways to characterise distance decay (*DD*),
the relationship between the distance
of a trip and the probability of it being made by a particular mode --- in this
case by bicycle. In this document we see how *DD* can be estimated
using flow data from the Census. Clearly
(as illustrated by the growth of long-distance bicycle touring) *all* trips
are cyclable in theory. In practice, however, short trips are far more likely
to be cycled than longer trips for reasons including time, stamina and comfort.
This fact is represented in the flow data (Figure 1). The challenge is extracting the
nature of *DD* from the flow data, which is rather opaque in its raw form.

```{r, message=FALSE, fig.cap="Plot of the demonstration city (Leeds) with all cycle trip flows represented by the white lines (oppacity is proportional to bicycle commuters).", echo=FALSE}
source("case-studies/leeds.R")
img <- readPNG("bigdata/figures/leeds-flow-bicycle.png")
grid.raster(img)
```

Building on 'the principle of parsimony' --- otherwise known as Occam's razor ---
we will start with the simplest model that explains *DD* in active
travel reasonably well This is log-linear decay, characterised by two parameters
(Iacono, 2010). We will build on this example to explore alternative
distance-decay functions.

# Estimating distance decay

A problem with the log-linear function
(and any log-model) for fitting to raw flow data on cycling is that
the log of zero is minus infinity ($log(0) = -\infty$) yet 0 flows between
origins and destinations are common for all modes of transport, and especially
so for rare modes. Of the 589 positive commuter flow lines between 25 MSOA zones 
surrounding Leeds city centre, for example, more than half (389) have 0 cyclists.
Zero and one-inflation
clearly poses a problem to efforts to parameterise *DD* through linear
regression: it is hard to estimate the parameters that result in infinity!

In addition, because flow matrices are so sparse, counts of 1 are common.
Thus both extreme values of
0 and 1 values are excessively common (or *inflated*) in raw flow data, 
as illustrated in Figure 2. This means that fitting non-probabilistic
regression models to the flow-level is problematic.

```{r, fig.height=3, fig.width=4, fig.cap="Scatterplot of flow distance and proportion cycling illustrating the frequency of 0s and 1s in the proportion of commutes by bicycle. Data: Leeds (n = 10,242).", echo=FALSE}
flow$pcycle <- flow$Bicycle / flow$All
plot(flow$dist, flow$pcycle, xlab = "Distance (km)", ylab = "Proportion of commutes by bike") 
# the problem with flow data: many 0's and 1's
```

There are at least two viable solutions to this problem:

1. Aggregating flows into distance bins such so that we estimate the *average*
(or population-weighted average) proportion of trips made by bicycle.
2. Zero-inflated regression, whereby the high frequency of zeros in the dependent
variable (% commuting by bicycle) is accounted for in the model.

Both solutions have advantages. Binning greatly reduces the computational requirements
of the model by grouping hundreds of distances into only a few dozen bins (how
many bins depends on the bin size). Binning is therefore
useful for parameter estimation. However, the binning approach cannot predict
the rate of flow between individual OD pairs, so cannot realistically be
used to estimate *ECP* for OD flows. Zero-inflated regression, by contrast,
can estimate OD-flows but is more computationally intensive.

## Binning flows by distance

### Binning distances

Distance in the flow data is calculated as the Euclidean distance
(a refinement would be to use route distance)
between origin and destination centroids and is a truly continuous variable.
This differs from the individual-level survey data which treats distance
as a quasi-continuous variable with a granularity of 0.1 miles (and clumping
around integer distances, which impacts on our selection of distance bins).
We bin the distances as follows, and set the maximum distance to 20.5 miles:

```{r}
brks <- c(0, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 9.5, 12.5, 15.5, 20.5)
flow <- flow[flow$dist < 20.5, ]
```

This allows for the OD flows to be grouped by distance and for the creation
of aggregates for each distance band:

```{r}
flow$binned_dist <- cut(flow$dist, breaks = brks, include.lowest = T)

# Create aggregate variables
gflow <- group_by(flow, binned_dist) %>%
  summarise(dist = mean(dist), mbike = mean(pcycle),
    total = sum(All))
gflow$nbike <- gflow$total * gflow$mbike / 100 # number of bikes in each zone
```

### Defining average distances per bin

The simplest approach to setting the distance associated with each bin
score is to take the mid-point between the upper and lower value of each bin:

```{r}
brks_min <- head(brks, -1)
brks_max <- brks[-1]
(midpoint <- (brks_min +brks_max) / 2)
```

However this is an oversimplification: distance distributions are strongly
positively skewed so the average of distances within the further distance bands
are likely to be substantially lower than the mean. This is true of our case
study city (Figure 3), therefore we use the computed mean distance for each bin:

```{r, fig.height=3, fig.width=4, fig.cap="The relationship between bin midpoint and average Euclidean distance of trips within each bin."}
plot(gflow$dist, midpoint, xlab = "Mean distance (km)")
abline(a = 0, b = 1)
```

The overall number of trips per distance band is illustrated in Figure 4.
The barely visible grey fill at the bottom of the bars represents bicycle
trips. Unsurprisingly, these all-but disappear after 6 km, demonstrating
that distance decay for cycling is faster than for trips overall.

```{r, echo=FALSE, fig.cap="Number of trips overall per distance band (white) and number of bicycle trips per distance band (grey)."}
tb <- sum(gflow$total)
ggplot(data = gflow) + 
  geom_histogram(aes(x = midpoint, y = ..density.. * 234968, weight = total),
  position = "identity", breaks = brks, fill = "white", colour = "black") +
    geom_histogram(aes(x = midpoint, y = ..density.. * 4648, weight = nbike),
  position = "identity", breaks = brks, alpha = 0.2) +
  ylab("Number of trips per bin") + xlab("Distance (km)") +
  theme_bw()
```

### The relationship between distance and cycling in grouped data

Now the data have been grouped and assigned to approximate distances,
we can use regression to explore **DD** for cycling.
The relationship is strikingly curvilinear in the example city.
With this non-linear relationship in mind (with a peak around 3 km),
we fitted various log-linear, quadratic and cubic functional forms to the data.

```{r}
mod_loglin <- lm(log(mbike) ~ dist, data = gflow)
mod_logsqr <- lm(log(mbike) ~ dist + I(dist^2), data = gflow)
mod_logcub <- lm(log(mbike) ~ dist + I(dist^2) + I(dist^3), data = gflow)
mod_cub <- lm(mbike ~ dist + I(dist^2) + I(dist^3), data = gflow)
```

The above code fitted log-linear, log-square and log-cubic functional forms to the
data. The following code runs **weighted regression**.
Adding weights to the regression in this way reduces the impact of
the longer-distance commutes, which fewer cyclists make. Note that two weight
vectors are used: total number of trips and total number of cyclists.
The results of these regressions are presented in Figure 5.

```{r}
modw_logcub <- lm(log(mbike) ~ dist + I(dist^2) + I(dist^3),
  weights = total, data = gflow)
modw2_logcub <- lm(log(mbike) ~ dist + I(dist^2) + I(dist^3),
  weights = nbike, data = gflow)
```

```{r, fig.cap="Linear regression to estimate *DD* from flow data. Blue, red and green lines represent log-linear, quadratic and cubic functional forms, respectively. The dotted and dashed green lines represent the impact of weighting but the number of total and cyclist commuter trips in each category, respectively.  The black line is the cubic regression, without taking the log of y.", echo=FALSE}
plot(gflow$dist, gflow$mbike,
  xlab = "Distance (km)", ylab = "Proportion cycling", ylim = c(0, 0.03))
lines(gflow$dist, exp(mod_loglin$fitted.values), col = "blue")
lines(gflow$dist, exp(mod_logsqr$fitted.values), col = "red")
lines(gflow$dist, exp(mod_logcub$fitted.values), col = "green")
lines(gflow$dist, exp(modw_logcub$fitted.values), col = "green", lty = 2)
lines(gflow$dist, exp(modw2_logcub$fitted.values), col = "green", lty = 3)
lines(gflow$dist, mod_cub$fitted.values, col = "black", lty = 4)
```

Does the log-term make any difference to model fit? A little, but not much
for the Leeds dataset.

From visual inspection of the models, the log-cubic model clearly fits better.
This is also apparent from the adjusted R-squared scores of each model
(which takes accound of over-fitting through excessive free parameters):
The log-cubic model explains around 90% of variation in the average
proportion of people who cycle in different distance groups:

```{r, eval=FALSE, echo=FALSE}
mnames <- ls()[grep("mod_", ls())]
mnames <- mnames[-grep("logi", mnames)]
mods <- data.frame(Model = mnames, Adj.R.sq = NA)
for(i in 1:nrow(mods)){
  mods$Adj.R.sq[i] <- summary(get(mnames[i]))$adj.r.squared
  # round(summary(get(mnames[i]))$coef, digits = 3 )[,1]
}
kable(mods, digits = 3)
```

|Model      | Adj.R.sq|
|:----------|--------:|
|mod_cub    |    0.784|
|mod_ind1   |    0.013|
|mod_ind2   |    0.041|
|mod_ind3   |    0.041|
|mod_logcub |    0.907|
|mod_loglin |    0.591|
|mod_logsqr |    0.569|

```{r, echo=FALSE}
# summary(modw_logcub)
# plot(gflow$dist, gflow$mbike,
#   xlab = "Distance (km)", ylab = "Proportion cycling")
# lines(gflow$dist, exp(modw_loglin$fitted.values), col = "blue")
# lines(gflow$dist, exp(modw_logsqr$fitted.values), col = "red")
# ```{r, fig.cap="Weighted regression results to estimate *DD* from flow data. Blue, red and green lines represent log-linear, quadratic and cubic functional forms, respectively."}
# old <- setwd("../")
# img <- readPNG("figures/weighted-reg.png")
# grid.raster(img)
# ```
```


```{r, echo=FALSE}

# 
# 
# ## Zero-inflated regression
```

### Estimating propensity to cycle based on individual flows

There are issues with fitting the regression model to the grouped data.
Using grouped data precludes the inclusion of flow-level variables that could
be useful in acurately estimating the potential level of cycling (CLC)
such as steepness and the characteristics of the origin zone.

```{r, echo=FALSE}
flow$clc <- flow$Bicycle / flow$All
mod_ind1 <- lm(pcycle ~ dist + I(dist^2) + I(dist^3), data = flow)
mod_ind2 <- lm(pcycle ~ dist + I(dist^2) + I(dist^3), data = flow,
  weights = All)
f3 <- flow[flow$dist < 15, ]
mod_ind3 <- lm(pcycle ~ dist + I(dist^2) + I(dist^3), data = f3,
  weights = All)
summary(mod_ind1)
df <- data.frame(dist = seq(0.1, max(flow$dist), length.out = 100))
df2 <- data.frame(dist = seq(0.1, max(flow$dist), length.out = 100))
```

We can fit exactly the same model to the individual-level data 

```{r, fig.cap="Distance decay curves based on the cubic polynomial regression against distance for all flows (red lines). The continuous and dashed lines represent unweighted, and weighted implementations. The dotted lines is fitted only to flows of < 15 km. The green line is the fit produced from the binned data."}

plot(flow$dist, flow$pcycle, xlim = c(0, 16), ylim = c(0, 0.04),
  xlab = "Distance (km)", ylab = "Percentage cycling")
lines(df$dist, predict(object = mod_ind1, df), col = "red", lwd = 4)
lines(df$dist, predict(object = mod_ind2, df), col = "red", lwd = 6, lty = 2)
lines(df$dist, predict(object = mod_ind3, df), col = "red", lwd = 6, lty = 3)
lines(df2$dist, exp(predict(object = modw_logcub, df2)),
  col = "green", lwd = 5)
# lines(gflow$dist, mod_cub$fitted.values, col = "black", lty = 4, lwd = 6)
```

## Fitting a simplified Planck Law

The dotted curve in Figure 6 illustrates a problem with the cubic distance
decay function: after reaching a minimum value it lurches upwards. Thus, in
all versions of this model, the proportion of people cycling 20 km to work
is greater than the proportion cycling 10 km. The dotted line shows that, even
after weighting the flows so the more common shorter trips have a disproportionate
impact on the model, the relatively rare longer trips hold great sway over
the shape of the DD curve produced by the cubic functions. This reduces the
fit of the model and, worse, means that DD curves in regions with no long-distance
trips will be very different from those produced in regions with long-distance
trips, even if the underlying pattern is identical.

A function that always tends to zero as distance tends to infinity would be
preferable. An equation in which the dependent variable
rises rapidly and then decays gradually with an independent variable is the
'Planck Law':

A simplified version of this equation was developed after some experimentation
to fit to the data:

$$
pc= \frac{a (b + d)}{e^{k(b + d)}}
$$

In words, this means that *pc* increases (how rapidly depends on the value of *b*)
to some maximum value (dependent on *a*) before entering a exponential decay
phase, the gradient of which dominated by *k*.

This function can be easily written in R and fitted to the data using the
Nonlinear Least Squares function `nls`.
The function can also be fitted to the flow-level data.

```{r, echo=FALSE}
# The original form of the Planck-like dd function:
dd_planck_orig <- function(x, a, b, k){
  (a * (x^b)) / (exp(k * x))
}
```


```{r}
dd_planck <- function(x, a, b, k){
  (a * (b + x)) / (exp(k * (b + x)))
}

mod_plank1 <- nls(mbike ~ dd_planck(dist, a, b, k), weights = total,
  data = gflow, start = list(a = 0.1, b = 0.1, k = 0.1))
```


```{r, echo=FALSE}
mod_plank2 <- nls(pcycle ~ dd_planck(dist, a, b, k), weights = All,
  data = flow, start = list(a = 0.1, b = 0.1, k = 0.1))

mod_plank3 <- nls(mbike ~ dd_planck_orig(dist, a, b, k), weights = total,
  data = gflow, start = list(a = 0.1, b = 0.1, k = 0.1))
```

Figure 7 compares the model fit acheived with this 'Planck' function against
that acheived with the log-cubic function. It was found that, despite the anomolous
upward tick in the example data (which would benefit the log-cubic function),
the Planck-like function fits the data better. Non-adjusted R-squared values
of 0.85 and 0.89 were obtained from the Planck and log-cubic functions respectively.

```{r, echo=FALSE, fig.cap="Fitted distance decay curves. The dotted line represents modw_logcub; the continuous blude line represents the new function."}
x = seq(0.1, 20, by = 0.1)
df <- data.frame(dist = x)
plot(gflow$dist, gflow$mbike,
  xlab = "Distance (km)", ylab = "Proportion cycling", ylim = c(0, 0.03))
lines(df$dist, predict(mod_plank1, df), col = "blue", lwd = 4)
# lines(df$dist, predict(mod_plank3, df), col = "blue")
# lines(df$dist, predict(mod_plank2, df), col = "red")
# lines(df$dist, predict(object = mod_ind2, df), col = "red", lty = 2)
lines(df2$dist, exp(predict(object = modw_logcub, df2)),
  col = "blue", lty = 5)
# cor(mod_plank1$m$fitted(), gflow$mbike)^2
# cor(mod_plank3$m$fitted(), gflow$mbike)^2
# cor(modw_logcub$fitted, gflow$mbike)^2
```

## What the parameters of the Planck function mean

Figure 8 illustrates the impact of varying the parameters *a*, *b* and *k*
on the Planck function. It is shown that these parameters allow scenarios of
future changes in DD to be changed according to knowledge of growth in the
willingness to cycle long distances (decreases in k), shifts in the distance
of the peak rate of cycling (b) and proportional increases in cycling for all
distances.

```{r, echo=FALSE, fig.cap="Model experiments illustrating the impacts of altering a, b and k parameters of the 'Planck' distance decay function. For a and k the green lines represent increases of 25% of the original value derived from the Leeds dataset and the red lines represent decreases of 10%. For b they represent changes of +2 and -1 respectively."}
pars <- coef(mod_plank1)
mults_plus <- cbind(diag(3) * c(0.25, 2, 0.25) + 1, "Increase", c("a", "b", "k"))
mults_minus <- cbind(diag(3) * -c(0.1, 1, 0.1) + 1, "Decrease", c("a", "b", "k"))
mults_orig <- cbind(matrix(1, 3, 3), "Original", c("a", "b", "k"))
mults <- rbind(mults_minus, mults_plus, mults_orig)
dfp <- NULL
for(i in 1:9){
  ptemp <- as.numeric(mults[i, 1:3]) * pars
  dftemp <- data.frame(
    Distance = x,
    PC = dd_planck(x, a = ptemp[1], b = ptemp[2], k = ptemp[3]),
    Experiment = mults[i, 4],
    Parameter = mults[i, 5]
  )
  dfp <- rbind(dfp, dftemp)
}
ggplot(dfp, aes(x = Distance, y = PC, color = Experiment)) + 
  geom_line() +
  facet_wrap(~ Parameter) +
  scale_color_manual(values = c("red", "green", "black")) +
  theme_bw()
```




# Regression against the cumulative probability of cycling

To reduce the noise in the data arising from 0s and 1s, the rate of
cycling can be viewed as a cumulative distribution of trips with distance:

```{r}
fc <- flow[ order(flow$dist), ] 
head(fc$dist)
head(cumsum(fc$Bicycle)) # cumulative distances
fc$cumclc <- cumsum(fc$Bicycle) / sum(fc$Bicycle)
```

We can fit a  logistic regression model to this data, with the following
functional form:

```{r}
# plot(fc$dis, fc$cumclc)
mod_logist1 <- glm(cumclc ~ dist, family = "binomial",
  weights = fc$All, data = fc)
y1 <- mod_logist1$fitted.values

# lines(fc$dist, y1, col = "red", lwd = 4)
summary(mod_logist1) # how to convert these params to dd?
```

Note the quality of the fit and the simplicity of the data, compared
with the non-cumulative distribution of proportion cycling with distance.
The challenge is converting this into a distance decay function and
allowing for *skew* in the rate of uptake. These things can be represented
in a more general version of the logistic equation, commonly known as the 
Richard's equation [@Richards1959]:

$$y = b + \dfrac{t - b}{\left [1 + 10^{r(x_{mid} - x)} \right ]^s}$$

$b$ and $t$ are the bottom and top asymptotes, and $r$, $x_{mid}$ and $s$ are the maximum slope, the x-coordinate at the inflexion point and an asymetric coefficient, respectively.

In R, this equation can be written as follows:

```{r, fig.cap="5 parameter logistic model"}
log5p <- function(x, b = 0, t = 1, xmid = 0, r = 1, s = 1, l = 1){
  b + (t - b) / (1 + 10 ^(r * (xmid - l * x)))^s
}
x = seq(0, 20, 0.1)
ytest = log5p(x, 0, 1, 5, 1, 1)
# plot(x, y) # plot not shown
```

```{r, echo=FALSE}
# Demonstrate this function in action - what it depends on and all...
```

The differential of this equation is displayed below, demonstrating the
link between cumulative probability probability density. 

$$
 \mathrm{log}(10) \,r \,s\,\left( T - B\right) \,{10}^{r\,\left( xmid - x\right) }\,{\left( {10}^{r\,( xmid-x) }+1\right) }^{-s-1}
$$

Converting this equation into an R function, we can plot the differential of the
curve described above:

```{r}
log5p_diff <- function(x, b = 0, t = 1, xmid = 0, r = 1, s = 1){
  r * s * (t - b) * 10^(r * (xmid - x)) * (10^(r * (xmid - x)) + 1)^(-s-1)
}
dd1 <- log5p_diff(x, 0, 1, 5, 1, 1)
# plot(x, ytest)
# lines(x, dd1)
```

Now we have demonstrated the link between the 5 parameter logistic
model and its differential, we can make the important observations:

1. Fitting a logistic regression model to the cumulative probability of trips
being made by cycling can result in an estimate of distance decay.
2. The *differential* of the Richard's allows for the calculation of model
parameters that are meaningful in terms of the rate of distance decay
($r$) and the absolute rate of cycling.

Let's fit the Richard's equation to the data. The differential form
can be fit directly to the raw data, while the logistic growth form can
be fitted to the cumulative probability form. In theory, we should get the
same result for the model parameters for both regression models.

```{r}
nls_5p1 <- nls(cumclc ~ 1 / (1 + 10 ^(r * (xmid - dist))),
  data = fc, start = list(xmid = 1, r = 0.2), 
  weights = fc$All)
nls_5p2 <- nls(cumclc ~ 1 / (1 + 10 ^(r * (xmid - dist)))^s,
  data = fc, start = list(xmid = 1, r = 0.2, s = 0.2),
  control = list(warnOnly =T), 
  weights = fc$All)

y3 <- predict(nls_5p1, fc)
y4 <- predict(nls_5p2, fc)
```


```{r}
# Plot the regressions
plot(fc$dist, fc$cumclc, type = "line")
lines(x, ytest, col = "yellow")
lines(fc$dist, y1, col = "green")
lines(fc$dist, y3, col = "red")
lines(fc$dist, y4, col = "blue")
```

To convert from the logistic regression results to distance decay,
simply extract the coefficients:

```{r, echo=FALSE, eval=FALSE}
(c1 <- coef(nls_5p1))
(c2 <- coef(nls_5p2))
dd2 <- log5p_diff(x, xmid =  c1[1], r = c1[2])
dd3 <- log5p_diff(x, xmid =  c2[1], r = c2[2], s = c2[3])

plot(flow$dist, flow$pcycle, xlim = c(0, 16), ylim = c(0, 0.1),
  xlab = "Distance (km)", ylab = "Percentage cycling")
lines(x, dd2 * 100, col = "red", lwd = 4)
lines(x, dd3 * 100, col = "green", lwd = 4)
lines(df$dist, predict(object = mod_ind2, df), col = "red", lwd = 6, lty = 2)
points(gflow$binned_dist, gflow$mbike, col = "blue", pch = 22)
# nls_5p1 <- nls(pcycle ~ b + (t - b) / (1 + 10 ^(r * (xmid - dist)))^s, data = fc, start = list(b = 0, t = 1, xmid = 5, r = 2, s = 1))
```



```{r, echo=FALSE, fig.cap="Illustration of differential of the Richard's equation fitted to the data (solid black: unweighted; dotted black: weighted)."}
# # Testing
# set.seed(20)
# pcycle <- rnorm(20, mean = 0.1, sd = 5)
# pcycle <- pcycle[ pcycle > 0 ]
# pcycle <- pcycle[ order(pcycle, decreasing = T)]
# fc2 <- data.frame(dist = 1:length(pcycle), pcycle)

# gamma distribution
fc3 <- fc
fc3$pcycle[fc3$pcycle == 0 ] <- 0.000001 # gamma dislikes zero values!
fc3$pcycle <- fc3$pcycle / 100
glm1 <- glm(pcycle ~ dist, family = Gamma(link = "identity"), data = fc3)
glm2 <- glm(pcycle ~ dist, family = Gamma("inverse"), data = fc3)

# gaussian
glm4 <- glm(pcycle ~ dist, family = gaussian, data = fc)

nls_5pd1 <- nls(pcycle ~ r * s * 10^(r * (dist)) * (10^(r * (dist)) + 1)^(-s-1), data = fc, start = list(r = 0.2, s = 1))

nls_5pd2 <- nls(pcycle ~ r * s * 10^(r * (dist)) * (10^(r * (dist)) + 1)^(-s-1), data = fc, start = list(r = 0.2, s = 1), weights = All)

# nls_5pd3 <- nls(pcycle ~ r * s * 10^(r * (dist * xmid)) * (10^(r * (dist * xmid)) + 1)^(-s-1), data = fc, start = list(xmid = 0.8, r = 0.2, s = 1), weights = All) 

nls_5pd4 <- nls(pcycle ~ r * s * 10^(r * (dist)^t) * (10^(r * (dist)) + 1)^(-s-1), data = fc, start = list(t = 1, r = 0.2, s = 1), weights = All)

nls_5pd5 <- nls(pcycle ~ r * s * 10^(r * (dist)) * (10^(r * (dist)) + 1)^(-s-1) + t, data = fc, start = list(t = 1, r = 0.2, s = 1), weights = All)

ynls1 <- predict(nls_5pd1, newdata = data.frame(dist = x)) 
ynls2 <- predict(nls_5pd2, newdata = data.frame(dist = x)) 
# ynls3 <- predict(nls_5pd3, newdata = data.frame(dist = x)) 
ynls4 <- predict(nls_5pd4, newdata = data.frame(dist = x)) 
ynls5 <- predict(nls_5pd5, newdata = data.frame(dist = x)) 

# Plotting
plot(fc$dist, fc$pcycle, xlab = "Distance (km)", ylab = "Proportion cycling", ylim = c(0, 0.04), xlim = c(0, 20))
lines(x, ynls1, lwd = 4, col = "red")
lines(x, ynls2, lwd = 4, lty = 2, col = "red")
# lines(x, ynls5, lwd = 4, lty = 2, col = "red")

# lines(x, ynls2, lwd = 4, lty = 3)
# lines(x, ynls4, lwd = 4, lty = 5) # no difference
lines(df$dist, predict(object = mod_ind1, df), col = "green", lwd = 4, lty = 2)
# lines(df$dist, predict(object = mod_ind2, df), col = "red", lwd = 2, lty = 2)
# lines(gflow$dist, exp(modw_logcub$fitted.values), col = "green", lty = 2, lwd = 6)
```

```{r}

# # # # # # # #
# Binned data #
# # # # # # # #

# nls_5pd1 <- nls(mbike ~ r * s * 10^(r * (dist)) * (10^(r * (dist)) + 1)^(-s-1), data = gflow, start = list(r = 0.1, s = 1))

nls_5pd2 <- nls(mbike ~ r * s * 10^(r * (dist)) * (10^(r * (dist)) + 1)^(-s-1), data = gflow, start = list(r = 0.2, s = 1), weights = total)

# nls_5pd3 <- nls(mbike ~ r * s * 10^(r * (dist * xmid)) * (10^(r * (dist * xmid)) + 1)^(-s-1), data = gflow, start = list(xmid = 0.8, r = 0.2, s = 1), weights = total) 

nls_5pd4 <- nls(mbike ~ r * s * 10^(r * (dist)^t) * (10^(r * (dist)) + 1)^(-s-1), data = gflow, start = list(t = 1, r = 0.2, s = 1), weights = total) 

nls_5pd5 <- nls(mbike ~ r * s * 10^(r * (dist)) * (10^(r * (dist)) + 1)^(-s-1) + t, data = gflow, start = list(t = 1, r = 0.2, s = 1), weights = total) 

# ynl1 <- predict(nls_5pd1, newdata = data.frame(dist = x)) 
ynl2 <- predict(nls_5pd2, newdata = data.frame(dist = x)) 
# ynl3 <- predict(nls_5pd3, newdata = data.frame(dist = x)) 
ynl4 <- predict(nls_5pd4, newdata = data.frame(dist = x)) 
ynl5 <- predict(nls_5pd5, newdata = data.frame(dist = x)) 

plot(gflow$dist, gflow$mbike,
  xlab = "Distance (km)", ylab = "Proportion cycling", ylim = c(0, 0.03))
# lines(x, ynl1, lwd = 4)
lines(x, ynl2, lwd = 4, lty = 2, col = "red")
# lines(x, ynls2, lwd = 4, lty = 3)
# lines(x, ynls4, lwd = 4, lty = 5) # no difference
lines(x, ynl5, lwd = 4, lty = 3, col = "blue") # no difference
# lines(df$dist, predict(object = mod_ind1, df), col = "red", lwd = 2)
# lines(df$dist, predict(object = mod_ind2, df), col = "red", lwd = 2, lty = 2)
lines(gflow$dist, exp(modw_logcub$fitted.values), col = "green", lty = 2, lwd = 3)
```


## Curve fitting with **nplr**

**nplr** is a package devoted to fitting the types of 'Richards'
curves, described above, to data [@Commo2014]. Below we see some examples
of the models that can be fitted, with
`npars` representing the number of parameters in the final model. 

```{r, eval=F}
mod_logist2 <- nplr(fc$dist, fc$cumclc)
mod_logist3 <- nplr(fc$dist, fc$cumclc, useLog = F)
mod_logist4 <- nplr(fc$dist, fc$cumclc, npars = 2)
mod_logist5 <- nplr(fc$dist, fc$cumclc, npars = 3)
plot(mod_logist4) 
```

```{r, echo=FALSE}
img <- readPNG("figures/2-param-nplr.png")
grid.raster(img)
```

The best in terms of simplicity seems to be the 4th model:

```{r, eval = F}
mod_logist4
# Instance of class nplr
# 
# 2-P logistic model
# Bottom asymptote: 0 
# Top asymptote: 1 
# Inflexion point at (x, y): 3.426146 0.5 
# Goodness of fit: 0.9947746 
# Standard error: 0.0205036 

getPar(mod_logist4)
# $npar
# [1] 2
# 
# $params
#   bottom top     xmid     scal s
# 1      0   1 3.426146 3.192967 1
```

Because only the $b$ and $x_{mid}$ parameters are used in the above regression,
the equation can be simplified substantially, to:

$$y = 1 + 10^{b(x_{mid} - x)} $$

```{r}
y =  1 / (1 + 10^(3.2 * (3.4 - x)))
plot(x, y)
```

# References



```{r, echo=FALSE}
# Bumpf (code for the rubbish bin :)
# log5p_formula <- formula(log5p_diff)
# nls_5pd1 <- nls(pcycle ~ r * s * 10^(r * (xmid - dist)) * (10^(r * (xmid - dist)) + 1)^(-s-1), data = fc, start = list( xmid = 5, r = 0.2, s = 10)) # fail!

# nls_5pd1 <- nls(pcycle ~ r * s * (t - b) * 10^(r * (xmid - dist)) * (10^(r * (xmid - dist)) + 1)^(-s-1), data = fc2, start = list(b = 0, t = 1, xmid = 5, r = -0.2, s = 0.1)) 
# nls_5p1 <- nls(pcycle ~ r * t * 10^(r * (xmid - dist)) * (10^(r * (xmid - dist)) + 1), data = fc, start = list(t = 1, xmid = 5, r = 2))
```



